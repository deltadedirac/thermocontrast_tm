import torch
from tqdm.auto import tqdm
import random
import math

class AdaptiveDataset(torch.utils.data.Dataset):
    
    def __init__(self,data, path_dir_esm2, path_dir_PiFold, emb_ensemble = 'single', type_emb='esm2', **kargs):
        self.data = data
        self.data.target = self.data.target.astype(float)
        self.emb_ensemble = emb_ensemble
        self.type_emb = type_emb
        self.path_dir_esm2 = path_dir_esm2
        self.path_dir_PiFold = path_dir_PiFold
        self.correspondence_idx2file = kargs['correspondence_idx']
        
        if 'sampler_use' in kargs:
            self.batch_samplerUse = kargs['sampler_use']
        else:
            self.batch_samplerUse = False #( False, kargs['sampler_use'] )[ 'sampler_use' in kargs ]             
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        #import ipdb; ipdb.set_trace()
        # if batch_sampler is enable, that means that the custome batch sampler will return you a pandas dataframe 
        # index, instead of the index generated by the length of the amount fo samples, i.e. I am not gonna get the index 
        # related to the position in the all structure (if structure has 1000 elements, the value will be from 0 - 999 )
        # but related to dataframe index (would be just related to those inside the indexing in the dataframe)
        #import ipdb; ipdb.set_trace()
        if self.batch_samplerUse:
            index = idx
        else:
            #label data
            index = self.data.index[idx]
        #sample, label = self.data.index[idx], self.data.target[idx] 
        sample, label = index, self.data.target[index]
        return sample, label
        #return self.data.index[idx], self.data.target
    
    def _mergeEmbeddings(self, idx, **kargs):
        
        max_pad_seq_size=kargs['max_pad_seq_size']
        max_pad_emb_dim=kargs['max_pad_emb_dim']
        
        if self.emb_ensemble != 'single':
            #tmp_esm2_emb =  torch.load(f"{self.path_dir_esm2}{self.data.index.get_loc(idx.item())}_{self.data.full_name[idx.item()]}.pt")
            #tmp_PiFold_emb = torch.load( f"{self.path_dir_PiFold}{self.data.index.get_loc(idx.item())}_{self.data.Original_ID[idx.item()]}.pt" )
            tmp_esm2_emb =  torch.load(f"{self.path_dir_esm2}{ self.correspondence_idx2file[idx.item()] }_{self.data.full_name[idx.item()]}.pt")
            tmp_PiFold_emb = torch.load( f"{self.path_dir_PiFold}{ self.correspondence_idx2file[idx.item()] }_{self.data.Original_ID[idx.item()]}.pt" )
            
            max_pad_seq_size = max( max_pad_seq_size, tmp_esm2_emb.shape[0], tmp_PiFold_emb.shape[0])
            max_pad_emb_dim = tmp_esm2_emb.shape[1] + tmp_PiFold_emb.shape[1] #max_pad_emb_dim, tmp_esm2_emb.shape[1], tmp_PiFold_emb[1])

            if 'emb_esm2' in kargs:
                kargs['emb_esm2'].append( tmp_esm2_emb )
            if 'emb_PiFold' in kargs:
                kargs['emb_PiFold'].append( tmp_PiFold_emb )
        else:        
            if self.type_emb == 'esm2':
                #tmp_esm2_emb =  torch.load(f"{self.path_dir_esm2}{self.data.index.get_loc(idx.item())}_{self.data.full_name[idx.item()]}.pt")
                tmp_esm2_emb =  torch.load(f"{self.path_dir_esm2}{ self.correspondence_idx2file[idx.item()] }_{self.data.full_name[idx.item()]}.pt")
                max_pad_seq_size = max( max_pad_seq_size, tmp_esm2_emb.shape[0])
                max_pad_emb_dim = tmp_esm2_emb.shape[1]
                kargs['emb_esm2'].append( tmp_esm2_emb )
            
            else:
                #tmp_PiFold_emb = torch.load( f"{self.path_dir_PiFold}{self.data.index.get_loc(idx.item())}_{self.data.Original_ID[idx.item()]}.pt" )
                tmp_PiFold_emb = torch.load( f"{self.path_dir_PiFold}{ self.correspondence_idx2file[idx.item()] }_{self.data.Original_ID[idx.item()]}.pt" )
                max_pad_seq_size = max( max_pad_seq_size, tmp_PiFold_emb.shape[0])
                max_pad_emb_dim = tmp_PiFold_emb.shape[1]
                kargs['emb_PiFold'].append( tmp_PiFold_emb )
        
        return max_pad_seq_size, max_pad_emb_dim
    
    def _fill_padded_matrix(self, matrix, a, b, batch_idx=0):
        
        #shape_a, shape_b  = a[batch_idx].shape, b[batch_idx].shape
        if batch_idx < matrix.shape[0]:
            if self.emb_ensemble != 'single':
                matrix[batch_idx, 0:a[batch_idx].shape[0], 0:a[batch_idx].shape[1]] = a[batch_idx]
                matrix[batch_idx, 0:b[batch_idx].shape[0], a[batch_idx].shape[1]:( a[batch_idx].shape[1] + b[batch_idx].shape[1] ) ] = b[batch_idx]
            else:
                tmp =  a[batch_idx] if len(a)!=0 else b[batch_idx]
                matrix[batch_idx, 0:tmp.shape[0], 0:tmp.shape[1]] = tmp

            self._fill_padded_matrix( matrix, a, b, batch_idx = batch_idx+1)
            #return matrix
                
            
    def getdata_fromQuery(self,idx_set):
        emb_esm2 = []
        emb_PiFold = []
        max_pad_seq_size = 0; max_pad_emb_dim=0
        #import ipdb; ipdb.set_trace()
        
        # fix the way to do the embeddings (to save), in order to avoid the for extra for-loop
        #for i in tqdm(idx_set, desc="loading individual embeddings:", leave=False):
        #print("loading individual embeddings.. \n")
        
        for i in idx_set:
            max_pad_seq_size, max_pad_emb_dim = self._mergeEmbeddings(i, emb_esm2=emb_esm2, emb_PiFold = emb_PiFold, 
                                                                      max_pad_seq_size = max_pad_seq_size , max_pad_emb_dim=max_pad_emb_dim)
        
        batch_ensembled = torch.zeros(len(idx_set), max_pad_seq_size, max_pad_emb_dim)
        
        self._fill_padded_matrix( batch_ensembled, emb_esm2, emb_PiFold)

        
        return batch_ensembled



class Species_BatchSample:
    
    """Stratified batch sampling
    Provides equal representation of target classes in each batch
    """
    def __init__(self, df, batch_size, shuffle=True):
        #import ipdb; ipdb.set_trace()#442 the amount of batches should be
        import math
        self.batch_size = batch_size
        self.df = df
        self.shuffle = shuffle
        species = df.organism.unique().tolist()
        #gg=df.organism.value_counts().tolist()
        self.length=sum( list(map(lambda el: math.ceil(el/51), df.organism.value_counts().tolist())) )
        #print(sum(ff))

    def __split__(self, list_a, chunk_size):
        for i in range(0, len(list_a), chunk_size):
            yield list_a[i:i + chunk_size]
    
    def _split_idx_per_species(self, species):
        df_species1=self.df[self.df.organism==species]
        idx_species=df_species1.index.tolist()
        if self.shuffle==True:
            random.shuffle(idx_species)
            
        return list(self.__split__(idx_species, self.batch_size))
    
    def _make_species_partitions(self):
        #import ipdb; ipdb.set_trace()
        idx_list_per_organism = []
        organism_list = self.df.organism.unique().tolist()
        
        for species in organism_list:
            idx_list_per_organism.extend(
                                            self._split_idx_per_species(species)   
                                         )
        
        if self.shuffle == True:
            random.shuffle(idx_list_per_organism)
        
        return idx_list_per_organism
        
    
    def __iter__(self):

        for species_idxs in self._make_species_partitions():
            yield species_idxs

    def __len__(self):
        return self.length#len(self.df)
    
    

from torch.utils.data import BatchSampler
from collections import defaultdict

class Species_BatchSample2:
    
    """Stratified batch sampling just ORDERING ACCORDING TO TAXONOMY, 
    AND DELIVER BATCHES MAJORITARY TO ONE SPECIES BUT WITH SOME PART OF CONTAMINATION
    IN SOME BATCHES
    Provides equal representation of target classes in each batch
    """
    def __init__(self, df, batch_size, shuffle=True):
        #import ipdb; ipdb.set_trace()#442 the amount of batches should be
        
        self.batch_size = batch_size
        self.df = df.sort_values(by=['organism'])
        #self.shuffle = shuffle
        species = df.organism.unique().tolist()


    def __split__(self, list_a, chunk_size):
        for i in range(0, len(list_a), chunk_size):
            yield list_a[i:i + chunk_size]
    
    def _split_idx_per_species(self, idx_lists):
        idx_species=idx_lists
        return list(self.__split__(idx_species, self.batch_size))        
    
    def __iter__(self):
        idx_ordered_species = self.df.organism.index.tolist()
        for species_idxs in self._split_idx_per_species(idx_ordered_species):
            yield species_idxs

    def __len__(self):
        return math.ceil( len(self.df)/self.batch_size )
    
import copy
class Species_Balancing_BatchSampler(BatchSampler):
    def __init__(self, dataframe, batch_size, shuffle=True, tag_search= 'organism', drop_last=False):
        
        self.dataframe = dataframe
        self.batch_size = batch_size
        self.drop_last = drop_last
        self.shuffle = shuffle
        self.species_indices = defaultdict(list)
        self.num_categories = len(dataframe[tag_search].unique())
        
        if shuffle:
            self.dataframe = self.shuffling(dataframe)
        # Group indices by species
        #import ipdb; ipdb.set_trace()
        
        for name, value in dataframe.groupby(tag_search):
            self.species_indices[name]=value

        #import ipdb; ipdb.set_trace()
        self.main_source = copy.deepcopy(self.species_indices)
        total_samples = len(dataframe)
        self.num_batches = (total_samples + batch_size - 1) // batch_size
        self.min_samples_per_category = self.batch_size // dataframe[tag_search].unique().shape[0]
        
    def shuffling(self, df):
        import numpy as np
        # Shuffle the index of the DataFrame
        shuffled_index = np.random.permutation(df.index)

        # Reindex the DataFrame with the shuffled index
        shuffled_df = df.reindex(shuffled_index)
        return shuffled_df
        
    def _query_sizes_species_indices(self, dtc):
        #import ipdb; ipdb.set_trace()
        status = dict()
        
        for key, value in dtc.items():
            status[key] = value.shape
        return status
    
    def _get_max_key_val(self, dct):
        
        # Custom key function to compare DataFrame sizes
        def dataframe_size(key_value_pair):
            key, df = key_value_pair
            return len(df)
        
        #import ipdb; ipdb.set_trace()
        return max(dct.items(), key=dataframe_size)[0]
    
    
    def _get_and_remove_vals_fromdicts(self, key,  diff):
        #get the vals from dataframe
        #import ipdb; ipdb.set_trace()
        val_max=self.species_indices[key][:diff]
        self.species_indices[key] = self.species_indices[key].iloc[diff:]
        
        # this is in case that after eliminating values from dataframes in the dictionary, 
        # the dataframe ends up empty which in that case must be delete such key, meaning 
        # that the filling of that category has been completed
        if self.species_indices[key].empty:
            del self.species_indices[key]
        return val_max
        
    def _flatten_list(self, nested_list):
        #import ipdb; ipdb.set_trace()
        import itertools
        #return list(itertools.chain(*nested_list))
        return list(itertools.chain.from_iterable(nested_list))

              
    def __iter__(self):
        
        #import ipdb; ipdb.set_trace
        batch = [] 
        
        self.species_indices = copy.deepcopy(self.main_source)
        
        for _ in range(self.num_batches):
            
            num_categories = len(self.species_indices.keys())
            min_samples_per_category = self.batch_size // num_categories
            
            diff_batch_vs_min_samples = self.batch_size - num_categories*min_samples_per_category
            
            if diff_batch_vs_min_samples !=0:
                #import ipdb; ipdb.set_trace()
            
                key_max = self._get_max_key_val(self.species_indices)
                val_max = self._get_and_remove_vals_fromdicts(key_max, 
                                    diff_batch_vs_min_samples + min_samples_per_category).index.values.tolist() 
                rest=list(self.species_indices.keys()); rest.remove(key_max)
                others= [ 
                         self._get_and_remove_vals_fromdicts(ii, min_samples_per_category)\
                                                                            .index.values.tolist()
                                                                             for ii in rest]
                
                batch.append( self._flatten_list( [val_max, *others] ) )
                #yield self._flatten_list( [val_max, *others] )
            else:
                #import ipdb; ipdb.set_trace
                rest=list(self.species_indices.keys())
                
                """
                tmp = self._flatten_list( [  
                                            self._get_and_remove_vals_fromdicts(ii, min_samples_per_category).\
                                                                            index.values.tolist()
                                                                                                 for ii in rest] )
                yield self._flatten_list( tmp)                                                                              
                
                """
                batch.append( self._flatten_list([  self._get_and_remove_vals_fromdicts(ii, min_samples_per_category).\
                                                                            index.values.tolist() for ii in rest ])  )
                
                
        
    
        #import ipdb; ipdb.set_trace()
        for ii in batch:
            yield ii
             
    #def __len__(self):
    #    return math.ceil( len(self.dataframe)/self.batch_size )